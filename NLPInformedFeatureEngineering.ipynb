{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ce4dc1-22e5-4494-8065-7c6963a108d3",
   "metadata": {},
   "source": [
    "# Importing of All Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfcea6e-86c0-4470-beed-cde7d905b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db897e12-658c-446c-9c59-c505969cbc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6c03ca-f64d-4109-8078-26f2fbbea818",
   "metadata": {},
   "source": [
    "# Scenario: Creating a Set of Machine Learning Friendly Features from EHR Data to Predict Type 2 Diabetes Onset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42703833-311f-4f75-97db-6cee7e69c4c2",
   "metadata": {},
   "source": [
    "First we will load in the necessary data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d741e79-49e5-4388-ab9d-42ad714c9a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_file(filename):\n",
    "    print(f\"Loading data for {filename}\")\n",
    "    df = pd.concat([ # use pd.concat to append/concatenate the data for all states together into a single frame\n",
    "        pd.read_parquet(f\"https://dicbworkshops.s3.amazonaws.com/{output_dir}/parquet/{filename}\") # use read_csv to load the data from each output directory\n",
    "        for output_dir in tqdm(['output_hi', 'output_ma', 'output_tx', 'output_wa'], leave=True, position=0) # loop over each output directory\n",
    "    ])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37273424-1e27-4117-abdb-26f6a940b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the conditions\n",
    "conditions = load_data_for_file('conditions.parquet')\n",
    "# load in the observations\n",
    "observations = load_data_for_file('observations.parquet')\n",
    "# load in the medications\n",
    "medications = load_data_for_file('medications.parquet')\n",
    "# load in the procedures\n",
    "procedures = load_data_for_file('procedures.parquet')\n",
    "# load in the patients\n",
    "patients = load_data_for_file('patients.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be869af-ce43-479b-acc5-36159833248f",
   "metadata": {},
   "source": [
    "## Extracting Patients with Type 2 Diabetes Diagnoses\n",
    "For this exercise, we are interested in studying patients with a diagnosis of Type-2 diabetes \\\n",
    "We select these from the conditions table based on the SNOMED code `44054006`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee57e11-031d-4ef6-b1c2-c93aa1db6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "type2_code = 44054006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58311aa1-17be-4766-b98d-a86a91fb4610",
   "metadata": {},
   "source": [
    "## Split the Data into 80/20 Training/Hold out Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f0c62-d9f3-4ded-8760-6cabfb6e2aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "type2_patients = conditions.query('CODE == @type2_code')['PATIENT'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf512d7f-f187-4d00-835e-423f5e558758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label the patients dataframe based on type2/non-type2 status\n",
    "patients_labeled = patients.assign(\n",
    "    label=lambda x: x['Id'].isin(type2_patients).astype('int')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a9afc7-9f7f-4d83-b9b2-2ebb11a24d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the patients into training and test sets\n",
    "patients_train, patients_test = train_test_split(patients_labeled, test_size=0.2, stratify=patients_labeled['label'], random_state=913)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57411571-f12d-4c4e-b6e8-e31966a74975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we split the rest of the files into train and test sets based on these patient splits\n",
    "conditions_train, conditions_test = (\n",
    "    conditions[conditions['PATIENT'].isin(patients_train['Id'])],\n",
    "    conditions[conditions['PATIENT'].isin(patients_test['Id'])]\n",
    ")\n",
    "observations_train, observations_test = (\n",
    "    observations[observations['PATIENT'].isin(patients_train['Id'])],\n",
    "    observations[observations['PATIENT'].isin(patients_test['Id'])]\n",
    ")\n",
    "medications_train, medications_test = (\n",
    "    medications[medications['PATIENT'].isin(patients_train['Id'])],\n",
    "    medications[medications['PATIENT'].isin(patients_test['Id'])]\n",
    ")\n",
    "procedures_train, procedures_test = (\n",
    "    procedures[procedures['PATIENT'].isin(patients_train['Id'])],\n",
    "    procedures[procedures['PATIENT'].isin(patients_test['Id'])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3cb00d-95bc-49f1-974c-2d1d32ded3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now collate these into dictionaries for later processing\n",
    "train_data = {\n",
    "    'conditions': conditions_train,\n",
    "    'observations': observations_train,\n",
    "    'medications': medications_train,\n",
    "    'procedures': procedures_train\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "    'conditions': conditions_test,\n",
    "    'observations': observations_test,\n",
    "    'medications': medications_test,\n",
    "    'procedures': procedures_test\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a693a265-6e61-4f50-af23-3722961294a9",
   "metadata": {},
   "source": [
    "# Definition of Helper Functions Used to Compose the Larger Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4040ce-a5e7-4eba-b536-c8111eb95587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to construct a lookup table of condition onset dates for patients based on the conditions table and a provided code\n",
    "def get_patient_onset_dates(conditions, code):\n",
    "    patients_with_condition = (\n",
    "        conditions.query('CODE == @code') # get all patients diagnosed with the code\n",
    "        .sort_values(by=['PATIENT', 'START']) # sort the data by patient ID and then start date\n",
    "        .drop_duplicates(subset=['PATIENT', 'START'], keep='first') # drop duplicates, keeping the instance with the earliest start date\n",
    "    )\n",
    "\n",
    "    # now build a lookup table/dictionary to map each patient's ID to the date of their earliest onset\n",
    "    patient_onset_dates = {\n",
    "        row['PATIENT']: row['START']\n",
    "        for _, row in patients_with_condition.iterrows()\n",
    "    }\n",
    "    return patient_onset_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9fc2f5-32fc-440b-84dc-6a35812d7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to construct simplified date columns for the observations, medications, and procedures tables\n",
    "def get_simplified_data(df, date_col, simplified_col='DATE_SIMPLE'):\n",
    "    return df.assign(**{\n",
    "        simplified_col: lambda x: pd.to_datetime(x[date_col]).dt.date.astype('str')\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231a045-9582-4721-9811-b5849619709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter out post diagnosis records from a table based on patient onset dates\n",
    "def filter_data_by_onset_dates(df, patient_onset_dates, date_column='DATE_SIMPLE'):\n",
    "    data_filtered = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), position=0, leave=True):\n",
    "        patient = row['PATIENT']\n",
    "        date = row[date_column]\n",
    "        if patient in patient_onset_dates and patient_onset_dates[patient] > date:\n",
    "            data_filtered.append(row)\n",
    "    return pd.DataFrame(data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ed7b1-8ce0-4464-8081-c623af3a7b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to unify the records for the four different types of events/ecounters into a single table\n",
    "def get_unified_records(conditions, observations, medications, procedures):\n",
    "    return pd.concat([\n",
    "        conditions[['PATIENT', 'START', 'CODE', 'DESCRIPTION']].assign(\n",
    "            EVENT_TYPE='CONDITION',\n",
    "        ).rename(columns={'START': 'DATE'}),\n",
    "        observations[['PATIENT', 'DATE_SIMPLE', 'CODE', 'DESCRIPTION']].assign(\n",
    "            EVENT_TYPE='OBSERVATION',\n",
    "        ).rename(columns={'DATE_SIMPLE': 'DATE'}),\n",
    "        medications[['PATIENT', 'DATE_SIMPLE', 'CODE', 'DESCRIPTION']].assign(\n",
    "            EVENT_TYPE='MEDICATION',\n",
    "        ).rename(columns={'DATE_SIMPLE': 'DATE'}),\n",
    "        procedures[['PATIENT', 'DATE_SIMPLE', 'CODE', 'DESCRIPTION']].assign(\n",
    "            EVENT_TYPE='PROCEDURE',\n",
    "        ).rename(columns={'DATE_SIMPLE': 'DATE'})\n",
    "    ]).sort_values(by=['PATIENT', 'DATE']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0726e8ca-c60f-4e17-ba31-425ed3188211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get condensed record data from unified records\n",
    "def get_condensed_record_data(unified_records):\n",
    "    # condense the records into a pipe-delimited string of event tokens per patient, \n",
    "    # where each token is of the form <EVENT_TYPE>::<CODE>\n",
    "    records_condensed = unified_records.assign(\n",
    "        EVENT_TOKEN=lambda x: x['EVENT_TYPE'] + '::' + x['CODE'].astype(str) + '|'\n",
    "    ).groupby(['PATIENT'])['EVENT_TOKEN'].sum().reset_index()\n",
    "    records_condensed['EVENT_TOKEN'] = records_condensed['EVENT_TOKEN'].str.rstrip('|')\n",
    "    return records_condensed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f96b5c-7879-4eae-a609-30666f04af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get condensed record data from unified records for n-gram input representation\n",
    "def get_condensed_record_data_for_ngrams(unified_records):\n",
    "    # condense the records into a space-delimited string of event descriptions per patient\n",
    "    records_condensed = unified_records.assign(\n",
    "        EVENT_DESCRIPTION=lambda x: x['DESCRIPTION'] + ' '\n",
    "    ).groupby(['PATIENT'])['EVENT_DESCRIPTION'].sum().reset_index()\n",
    "    records_condensed['EVENT_DESCRIPTION'] = records_condensed['EVENT_DESCRIPTION'].str.rstrip()\n",
    "    return records_condensed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e240ec8-7181-43d5-98e7-cda446830e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to vectorize the unified record data into binary occurence format\n",
    "def get_multihot_vector_representation(condensed_records, vectorizer, feature_col='EVENT_TOKEN', train=True):\n",
    "    # now get the multi-hot representation from the vectorizer\n",
    "    if train:\n",
    "        # if this is the training set, fit before transforming\n",
    "        return vectorizer.fit_transform(condensed_records[feature_col])\n",
    "    else:\n",
    "        #otherwise, just transform\n",
    "        return vectorizer.transform(condensed_records[feature_col])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac3968-48ac-4d89-a777-2f06a4c059e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add patient ages to a given dataframe, and compute age bins from those ages\n",
    "def get_aged_patient_data(events_df, patients_df, bin_width=5, date_col='DATE_SIMPLE'):\n",
    "    # first merge in the birthdates from the patients dataframe\n",
    "    merged = events_df.merge(\n",
    "        patients_df[['Id', 'BIRTHDATE']],\n",
    "        left_on='PATIENT',\n",
    "        right_on='Id'\n",
    "    )\n",
    "    # now calculate the age from the birthdate and the date column\n",
    "    aged = merged.assign(\n",
    "        AGE=lambda x: (pd.to_datetime(x[date_col]) - pd.to_datetime(x['BIRTHDATE'])).dt.days // 365\n",
    "    )\n",
    "    # now use the calculated age to compute age bins using pd.cut\n",
    "    age_binned = aged.assign(\n",
    "        AGE_BIN=lambda x: pd.cut(x['AGE'], bins=list(np.arange(0, x['AGE'].max() + bin_width, bin_width)), include_lowest=True)\n",
    "    )\n",
    "    # now get the human readable age bin\n",
    "    result = age_binned.assign(\n",
    "        AGE_RANGE=lambda x: x['AGE_BIN'].apply(lambda b: f\"{int(b.left)}-{int(b.right)}\" if b.left <= 0 else f\"{int(b.left)+1}-{int(b.right)}\")\n",
    "    )\n",
    "    # return the age_binned data\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b2407-aae4-4c6d-b5f8-52749461d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build age_binned distributions for numeric observations\n",
    "def get_age_binned_distributions(observations_df):\n",
    "    # first make sure that we are only considering numeric observations\n",
    "    numeric_obs = observations_df.query('TYPE == \"numeric\"')\n",
    "    # build the datastructure to store the distributions\n",
    "    observation_distributions = {\n",
    "        code: {}\n",
    "        for code in numeric_obs['CODE'].unique()\n",
    "    }\n",
    "    # now iterate over the observations, and construct the age binned distributions\n",
    "    for _, row in tqdm(numeric_obs.iterrows(), total=len(numeric_obs)):\n",
    "        # get the value, code, and age_range\n",
    "        value = row['VALUE']\n",
    "        code = row['CODE']\n",
    "        age_range = row['AGE_RANGE']\n",
    "        # add the value to the distribution for the corresponding code and age range\n",
    "        observation_distributions[code][age_range] = observation_distributions[code].get(age_range, []) + [value]\n",
    "    # now sort all of the distributions\n",
    "    for age_bin_dists in observation_distributions.values():\n",
    "        for age_bin_dist in age_bin_dists.values():\n",
    "            age_bin_dist.sort()\n",
    "    return observation_distributions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa0023-d9e9-4ecd-9751-69127b080483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast implementation of percentile ranking, assume group is sorted\n",
    "def fast_percentile_ranking(group, value):\n",
    "    if len(group) == 0:\n",
    "        return np.nan\n",
    "    return recursive_ranking(group, value, 0, len(group) - 1)\n",
    "\n",
    "def recursive_ranking(group, value, start, end):\n",
    "    if start > end:\n",
    "        return start / len(group) * 100\n",
    "\n",
    "    mid = (start + end) // 2\n",
    "\n",
    "    if value > group[mid]:\n",
    "        return recursive_ranking(group, value, mid + 1, end)\n",
    "    elif value < group[mid]:\n",
    "        return recursive_ranking(group, value, start, mid - 1)\n",
    "    else:\n",
    "        # Handle case where value == group[mid]\n",
    "        low, high = mid, mid\n",
    "\n",
    "        # Extend low to include all equal values before mid\n",
    "        while low > start and group[low - 1] == group[mid]:\n",
    "            low -= 1\n",
    "\n",
    "        # Extend high to include all equal values after mid\n",
    "        while high < end and group[high + 1] == group[mid]:\n",
    "            high += 1\n",
    "\n",
    "        low = low if group[low] == value else low + 1\n",
    "        high = high if group[high] == value else high - 1\n",
    "\n",
    "        # Calculate percentile for the range of identical values\n",
    "        low_percentile = (low+1) / len(group) * 100\n",
    "        high_percentile = (high+1) / len(group) * 100\n",
    "\n",
    "        # Average the percentiles of the range of identical values\n",
    "        return (low_percentile + high_percentile) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a992705-cfa5-4a00-956c-b23b41ed948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute the percentile score of all numeric observations against those in their age cohort\n",
    "def get_percentile_ranked_observations(observations_df, observation_distributions):\n",
    "    observations_ranked = observations_df.assign(\n",
    "        PERCENTILE_RANK=lambda x: x.progress_apply(\n",
    "            lambda row: fast_percentile_ranking(observation_distributions.get(row['CODE'], {}).get(row['AGE_RANGE'], []), row['VALUE']),\n",
    "            axis=1\n",
    "        )\n",
    "    )\n",
    "    return observations_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfc1be-5078-4281-8489-ac69afb5f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add the label for the percentile score of the ranked numeric observations to the CODE column\n",
    "def get_percentile_labeled_observations(observations_df, percentiles=4):\n",
    "    # make sure we drop any observations for which the percentile_rank is nan\n",
    "    observations_no_nan = observations_df.dropna(subset=['PERCENTILE_RANK']).reset_index(drop=True).copy()\n",
    "    # use the pd.cut function to bin the PERCENTILE_RANK into percentiles many bins\n",
    "    bin_width = 100 // percentiles\n",
    "    observations_no_nan['PERCENTILE'] = pd.cut(\n",
    "        observations_no_nan['PERCENTILE_RANK'], \n",
    "        bins=np.arange(0, 100+bin_width, bin_width), \n",
    "        include_lowest=True\n",
    "    )\n",
    "    observations_no_nan['CODE'] = (\n",
    "        observations_no_nan['CODE'] + '__' + \n",
    "        observations_no_nan['PERCENTILE'].map(lambda interval: interval.left).astype('int').astype('str') + \"th - \" +\n",
    "        observations_no_nan['PERCENTILE'].map(lambda interval: interval.right).astype('int').astype('str') +\n",
    "        'th Percentile'\n",
    "    )\n",
    "    return observations_no_nan\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ea42ae-6c98-4ace-9904-1c9b6994a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_classifier_kfold(clf, X, y, k=5):\n",
    "    metrics = []\n",
    "    kfold = StratifiedKFold(n_splits=k, random_state=913, shuffle=True)\n",
    "    for i, (train_index, test_index) in tqdm(enumerate(kfold.split(X, y)), total=k, position=0, leave=True):\n",
    "        train_x, train_y = X[train_index], y[train_index]\n",
    "        test_x, test_y = X[test_index], y[test_index]\n",
    "        # fit the model on the training fold\n",
    "        clf.fit(train_x, train_y)\n",
    "        # evaluate the model on the validation fold\n",
    "        preds = clf.predict(test_x)\n",
    "        scores = clf.predict_proba(test_x)[:, 1]\n",
    "        # get the AUROC\n",
    "        fpr, tpr, _ = roc_curve(test_y, scores)\n",
    "        auroc = auc(fpr, tpr)\n",
    "        # get the confusion matrix\n",
    "        cm = confusion_matrix(test_y, preds)\n",
    "        # save the metrics\n",
    "        metrics.append({\n",
    "            'AUROC': auroc,\n",
    "            'Precision': cm[1, 1] / cm[:, 1].sum(),\n",
    "            'Recall': cm[1, 1] / cm[1].sum(),\n",
    "            'Specificity': cm[0, 0] / cm[0].sum()\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics), fpr, tpr, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea89cf45-2cec-4d5f-8e2b-fe9768d50645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets feature importance rankings assuming a tree-based model as clf (DecisionTree, RandomForest, ExtraTrees, etc.)\n",
    "def get_feature_importance_rankings(clf, vectorizer, all_records):\n",
    "    reverse_lookup = {\n",
    "        value: key for key, value in vectorizer.vocabulary_.items()\n",
    "    }\n",
    "    importances = clf.feature_importances_\n",
    "    feature_names = [reverse_lookup[idx] for idx in np.arange(0, len(importances), 1)]\n",
    "    importances_df = pd.DataFrame({\n",
    "        'FEATURE_NAME': feature_names,\n",
    "        'FEATURE_IMPORTANCE': importances\n",
    "    }).assign(\n",
    "        CODE=lambda x: x['FEATURE_NAME'].str.split('::').apply(lambda pair: pair[1])\n",
    "    ).merge(\n",
    "        all_records[['CODE', 'DESCRIPTION']].drop_duplicates().astype({'CODE': str}),\n",
    "        on='CODE',\n",
    "    ).sort_values(by='FEATURE_IMPORTANCE', ascending=False)\n",
    "    return importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf60616-e1fd-4d94-81f0-e2be8b23b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance_rankings_lr(clf, vectorizer, all_records):\n",
    "    reverse_lookup = {\n",
    "        value: key for key, value in vectorizer.vocabulary_.items()\n",
    "    }\n",
    "    importances = clf.coef_[0]\n",
    "    feature_names = [reverse_lookup[idx] for idx in np.arange(0, len(importances), 1)]\n",
    "    importances_df = pd.DataFrame({\n",
    "        'FEATURE_NAME': feature_names,\n",
    "        'FEATURE_IMPORTANCE': importances,\n",
    "        'ABSOLUTE_IMPORTANCE': np.abs(importances)\n",
    "    }).assign(\n",
    "        CODE=lambda x: x['FEATURE_NAME'].str.split('::').apply(lambda pair: pair[1])\n",
    "    ).merge(\n",
    "        all_records[['CODE', 'DESCRIPTION']].drop_duplicates().astype({'CODE': str}),\n",
    "        on='CODE',\n",
    "    ).sort_values(by='ABSOLUTE_IMPORTANCE', ascending=False)\n",
    "    return importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f96a654-1b80-455c-93de-549467836914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance_rankings_ngram_lr(clf, vectorizer):\n",
    "    reverse_lookup = {\n",
    "        value: key for key, value in vectorizer.vocabulary_.items()\n",
    "    }\n",
    "    importances = clf.coef_[0]\n",
    "    feature_names = [reverse_lookup[idx] for idx in np.arange(0, len(importances), 1)]\n",
    "    importances_df = pd.DataFrame({\n",
    "        'FEATURE_NAME': feature_names,\n",
    "        'FEATURE_IMPORTANCE': importances,\n",
    "        'ABSOLUTE_IMPORTANCE': np.abs(importances)\n",
    "    }).sort_values(by='ABSOLUTE_IMPORTANCE', ascending=False)\n",
    "    return importances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffddece-071a-464f-bbaa-27ecc2e2532e",
   "metadata": {},
   "source": [
    "# Common Data Preprocessing Steps\n",
    "All of the pipelines that we look at today will depend on the same set of preprocessing steps, \\\n",
    "so we have broken those out into a separate function that can be run once so the processed data can \\\n",
    "be reused by all of the pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c2dffa-52ba-4adc-bd90-28b4b14e07f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_data(data):\n",
    "    # assign dictionary values to variables for less verbose access\n",
    "    conditions = data['conditions']\n",
    "    observations = data['observations']\n",
    "    medications = data['medications']\n",
    "    procedures = data['procedures']\n",
    "    \n",
    "    print(\"Getting onset dates...\")\n",
    "    # get onset dates of patients for type-2 diabetes with SNOMED code 44054006\n",
    "    type2_onset_dates = get_patient_onset_dates(conditions, type2_code)\n",
    "\n",
    "    print(\"Simplifying dates...\")\n",
    "    # add simplified date columns to the observations, medications, and procedures\n",
    "    observations_simple = get_simplified_data(observations, 'DATE')\n",
    "    medications_simple = get_simplified_data(medications, 'START')\n",
    "    procedures_simple = get_simplified_data(procedures, 'START')\n",
    "\n",
    "    # drop all cause of death observations from the data\n",
    "    observations_non_cod = observations_simple[observations_simple['CODE'] != '69453-9']\n",
    "\n",
    "    print(\"Filtering out postdiagnosis events...\")\n",
    "    # now we will get filtered data for the type 2 patients to exclude post-diagnosis information\n",
    "    conditions_filtered = filter_data_by_onset_dates(conditions, type2_onset_dates, 'START')\n",
    "    observations_filtered = filter_data_by_onset_dates(observations_non_cod, type2_onset_dates)\n",
    "    medications_filtered = filter_data_by_onset_dates(medications_simple, type2_onset_dates)\n",
    "    procedures_filtered = filter_data_by_onset_dates(procedures_simple, type2_onset_dates)\n",
    "\n",
    "    # now we will save the set of unique type2 patients who have a pre-diagnosis record\n",
    "    type2_patients = pd.concat([\n",
    "        conditions_filtered['PATIENT'],\n",
    "        observations_filtered['PATIENT'],\n",
    "        medications_filtered['PATIENT'],\n",
    "        procedures_filtered['PATIENT']\n",
    "    ]).unique()\n",
    "\n",
    "    # now extract the observations for the non-type2 patients\n",
    "    conditions_non_type2 = conditions[~conditions['PATIENT'].isin(type2_patients)]\n",
    "    observations_non_type2 = observations_non_cod[~observations_non_cod['PATIENT'].isin(type2_patients)]\n",
    "    medications_non_type2 = medications_simple[~medications_simple['PATIENT'].isin(type2_patients)]\n",
    "    procedures_non_type2 = procedures_simple[~procedures_simple['PATIENT'].isin(type2_patients)]\n",
    "\n",
    "    type2_data = {\n",
    "        'conditions': conditions_filtered,\n",
    "        'observations': observations_filtered,\n",
    "        'medications': medications_filtered,\n",
    "        'procedures': procedures_filtered\n",
    "    }\n",
    "\n",
    "    non_type2_data = {\n",
    "        'conditions': conditions_non_type2,\n",
    "        'observations': observations_non_type2,\n",
    "        'medications': medications_non_type2,\n",
    "        'procedures': procedures_non_type2\n",
    "    }\n",
    "\n",
    "    return type2_data, non_type2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b11fc9-762c-4bcd-a09e-f3dca40be180",
   "metadata": {},
   "outputs": [],
   "source": [
    "type2_data, non_type2_data = get_preprocessed_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86fc2dd-84e2-404f-a460-d37ab9fa3857",
   "metadata": {},
   "source": [
    "# Pipeline/Feature Engineering Alternative Specifications and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241e706-84fd-44f7-81b7-1cfa8902094f",
   "metadata": {},
   "source": [
    "## Option 1: Bag of Labeled Clinical Encounters (Many-hot/multi-hot encoding)\n",
    "The simplest feature representation we can create and test is a binary vector (many-hot/multi-hot) representation \\\n",
    "which encodes the occurence or lack-therof of different clinical encounters/event in each patient's EHR record \\\n",
    "To construct this representation, we can use the scikit-learn package's `CountVectorizer` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76c7e4-8681-414c-9b05-08b49a309c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_option1(type2_data, non_type2_data):\n",
    "    print(\"Unifying data for type 2 and non-type2 patients...\")\n",
    "    # now we will unify the data together into a single set of records\n",
    "    unified_records_type2 = get_unified_records(\n",
    "        type2_data['conditions'],\n",
    "        type2_data['observations'],\n",
    "        type2_data['medications'],\n",
    "        type2_data['procedures']\n",
    "    )\n",
    "\n",
    "    unified_records_non_type2 = get_unified_records(\n",
    "        non_type2_data['conditions'],\n",
    "        non_type2_data['observations'],\n",
    "        non_type2_data['medications'],\n",
    "        non_type2_data['procedures']\n",
    "    )\n",
    "    print(\"Converting to condensed representation...\")\n",
    "    # now we will condense the records for the type 2 and non-type 2 patients\n",
    "    type2_condensed = get_condensed_record_data(unified_records_type2)\n",
    "    non_type2_condensed = get_condensed_record_data(unified_records_non_type2)\n",
    "\n",
    "    # now we will concatenate the two datasets together and label them\n",
    "    all_data_condensed = pd.concat([\n",
    "        type2_condensed.assign(LABEL=1),\n",
    "        non_type2_condensed.assign(LABEL=0)\n",
    "    ])\n",
    "\n",
    "    print(\"Vectorizing data...\")\n",
    "    # now we will get the multi-hot vector representation for the records\n",
    "    vectorizer = CountVectorizer(\n",
    "        binary=True,\n",
    "        tokenizer=lambda x: x.split('|'),\n",
    "        token_pattern=None,\n",
    "        lowercase=False\n",
    "    )\n",
    "\n",
    "    # now vectorize the data to get a multi-hot representation\n",
    "    multi_hot_vectors = get_multihot_vector_representation(all_data_condensed, vectorizer)\n",
    "\n",
    "    # print out the dimensionality of the multi_hot_vectors\n",
    "    print(f\"Multi hot vectors have {multi_hot_vectors.shape[1]} features\")\n",
    "\n",
    "    print(\"Fitting and evaluating classifier...\")\n",
    "    # now we will construct the random forest classifier and evaluate it using KFold cross-validation\n",
    "    clf = LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "        max_iter=1000,\n",
    "        random_state=913\n",
    "    )\n",
    "\n",
    "    # now we train and evaluate the classifier\n",
    "    results, fpr, tpr, cm = train_and_evaluate_classifier_kfold(\n",
    "        clf, multi_hot_vectors, all_data_condensed['LABEL'].to_numpy()\n",
    "    )\n",
    "\n",
    "    # create and display the ROC curve plot\n",
    "    display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=results['AUROC'].iloc[-1], estimator_name='Logistic Regression - Pipeline 1')\n",
    "    display.plot()\n",
    "    plt.show()\n",
    "\n",
    "    # create and display the confusion matrix plot\n",
    "    display = ConfusionMatrixDisplay(cm, display_labels=np.array(['Non-Type 2', 'Type 2']))\n",
    "    display.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "\n",
    "    # now return the results, trained classifier, and vectorizer\n",
    "    return results, clf, vectorizer, pd.concat([unified_records_type2, unified_records_non_type2])\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6964c-d79f-46fb-b97d-7c01f8f5617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, clf, vectorizer, all_records = pipeline_option1(type2_data, non_type2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9dd161-a2b5-4dd9-9a77-0d3f7419e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde518bb-befb-4e38-8f76-9530b38ded6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['AUROC'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814e1ea-83f8-4231-919c-0ccc48d025c0",
   "metadata": {},
   "source": [
    "### Auditing the Model with Feature Importance Rankings\n",
    "One thing that we can do is audit the way our current model is behaving, and what its predictions are based \\\n",
    "on, by looking at feature importance rankings. Here we look at the values of the coefficients of the logistic regression model  \\\n",
    "to get a sense for how each binary feature is contributing to the log odds of the binary target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f524c4-edeb-4942-946a-f1f2c826f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = get_feature_importance_rankings_lr(clf, vectorizer, all_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a70046-1169-4d00-99dd-5b91a71b299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d167a96-d620-4074-a427-2f53e4748c37",
   "metadata": {},
   "source": [
    "## Option 2: Binary Occurrence with Inclusion of Discretized Numeric Features\n",
    "While the feature importance rankings revealed known co-morbidities (e.g., tooth loss) and known risk-factors for diabetes (e.g., Prediabetes, BMI) \\\n",
    "we have left out entirely the numeric data from the observations (lab and vital sign measures) which are likely to contain important information. \\\n",
    "Here we look at one technique for incorporating this information while maintaining the binary occurence vector representation that we used previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d4382-444b-4506-bcf0-42de01880d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_preprocessing_steps(type2_data, non_type2_data, observation_distributions=None):\n",
    "    print(\"Adding patient ages to observations...\")\n",
    "    ###### NEW STEP: Add ages to the observations data and compute age bins ######\n",
    "    observations_type2_aged = get_aged_patient_data(type2_data['observations'], patients)\n",
    "    observations_non_type2_aged = get_aged_patient_data(non_type2_data['observations'], patients)\n",
    "\n",
    "    observations_type2_numeric = observations_type2_aged.query('TYPE == \"numeric\"')\n",
    "    observations_non_type2_numeric = observations_non_type2_aged.query('TYPE == \"numeric\"')\n",
    "\n",
    "    if observation_distributions is None:\n",
    "        print(\"Computing age-binned numeric observation distributions...\")\n",
    "        ###### NEW STEP: Compute the age-binned numeric observation distributions ######\n",
    "        observation_distributions = get_age_binned_distributions(\n",
    "            pd.concat([observations_type2_numeric, observations_non_type2_numeric])\n",
    "        )\n",
    "\n",
    "    print(\"Computing percentile rank of numeric observations...\")\n",
    "    ###### NEW STEP: Compute the percentile rank of each observation against those for patients in the same age cohort ######\n",
    "    observations_type2_ranked = get_percentile_ranked_observations(observations_type2_numeric, observation_distributions)\n",
    "    observations_non_type2_ranked = get_percentile_ranked_observations(observations_non_type2_numeric, observation_distributions)\n",
    "\n",
    "    observations_type2_labeled = get_percentile_labeled_observations(observations_type2_ranked)\n",
    "    observations_non_type2_labeled = get_percentile_labeled_observations(observations_non_type2_ranked)\n",
    "\n",
    "    ###### Make sure we recombine the numeric and non-numeric observations ######\n",
    "    observations_type2_final = pd.concat([\n",
    "        observations_type2_labeled.drop(columns=['PERCENTILE', 'PERCENTILE_RANK']),\n",
    "        observations_type2_aged.query('TYPE != \"numeric\"')\n",
    "    ])\n",
    "\n",
    "    observations_non_type2_final = pd.concat([\n",
    "        observations_non_type2_labeled.drop(columns=['PERCENTILE', 'PERCENTILE_RANK']),\n",
    "        observations_non_type2_aged.query('TYPE != \"numeric\"')\n",
    "    ])\n",
    "\n",
    "    type2_data_processed = {\n",
    "        key: value if key != 'observations' else observations_type2_final\n",
    "        for key, value in type2_data.items()\n",
    "    }\n",
    "\n",
    "    non_type2_data_processed = {\n",
    "        key: value if key != 'observations' else observations_non_type2_final\n",
    "        for key, value in non_type2_data.items()\n",
    "    }\n",
    "\n",
    "    return type2_data_processed, non_type2_data_processed, observation_distributions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef567f8-5778-4fca-941f-f5e42115fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "type2_data_p2, non_type2_data_p2, observation_distributions = additional_preprocessing_steps(type2_data, non_type2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e752d-7a55-4b9a-a19a-7f98d0c8e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_option2(type2_data, non_type2_data):\n",
    "\n",
    "    print(\"Unifying and condensing records...\")\n",
    "    # now we will unify the data together into a single set of records\n",
    "    unified_records_type2 = get_unified_records(\n",
    "        type2_data['conditions'],\n",
    "        type2_data['observations'],\n",
    "        type2_data['medications'],\n",
    "        type2_data['procedures']\n",
    "    )\n",
    "\n",
    "    unified_records_non_type2 = get_unified_records(\n",
    "        non_type2_data['conditions'],\n",
    "        non_type2_data['observations'],\n",
    "        non_type2_data['medications'],\n",
    "        non_type2_data['procedures']\n",
    "    )\n",
    "\n",
    "    # now we will condense the records for the type 2 and non-type 2 patients\n",
    "    type2_condensed = get_condensed_record_data(unified_records_type2)\n",
    "    non_type2_condensed = get_condensed_record_data(unified_records_non_type2)\n",
    "\n",
    "    # now we will concatenate the two datasets together and label them\n",
    "    all_data_condensed = pd.concat([\n",
    "        type2_condensed.assign(LABEL=1),\n",
    "        non_type2_condensed.assign(LABEL=0)\n",
    "    ])\n",
    "\n",
    "    print(\"Vectorizing data...\")\n",
    "    # now we will get the multi-hot vector representation for the records\n",
    "    vectorizer = CountVectorizer(\n",
    "        binary=True,\n",
    "        tokenizer=lambda x: x.split('|'),\n",
    "        token_pattern=None,\n",
    "        lowercase=False\n",
    "    )\n",
    "\n",
    "    # now vectorize the data to get a multi-hot representation\n",
    "    multi_hot_vectors = get_multihot_vector_representation(all_data_condensed, vectorizer)\n",
    "\n",
    "    print(f\"Multi hot vectors have {multi_hot_vectors.shape[1]} features\")\n",
    "\n",
    "    print(\"Fitting and evaluating classifier...\")\n",
    "    # now we will construct the random forest classifier and evaluate it using KFold cross-validation\n",
    "    clf = LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "        max_iter=1000,\n",
    "        random_state=913\n",
    "    )\n",
    "\n",
    "    # now we train and evaluate the classifier\n",
    "    results, fpr, tpr, cm = train_and_evaluate_classifier_kfold(\n",
    "        clf, multi_hot_vectors, all_data_condensed['LABEL'].to_numpy()\n",
    "    )\n",
    "\n",
    "    # create and display the ROC curve plot\n",
    "    display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=results['AUROC'].iloc[-1], estimator_name='Logistic Regression - Pipeline 2')\n",
    "    display.plot()\n",
    "    plt.show()\n",
    "\n",
    "    # create and display the confusion matrix plot\n",
    "    display = ConfusionMatrixDisplay(cm, display_labels=np.array(['Non-Type 2', 'Type 2']))\n",
    "    display.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "\n",
    "    # now return the results, trained classifier, and vectorizer\n",
    "    return results, clf, vectorizer, pd.concat([unified_records_type2, unified_records_non_type2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd8248-5711-47c4-83c3-801f50d676ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p2, clf_p2, vectorizer_p2, all_records_p2 = pipeline_option2(type2_data_p2, non_type2_data_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc9911-77ce-46c1-ac48-3739a4ffcb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313f0d5-0dd9-4a50-bd17-712c87ff6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p2['AUROC'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f855f-fea0-4662-8596-f2049e7bcf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_p2 = get_feature_importance_rankings_lr(clf_p2, vectorizer_p2, all_records_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91656b72-7c26-4b34-b25a-60d2a9605b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_p2.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab79e3c8-2a92-4cb8-91e2-4b5018b30ef2",
   "metadata": {},
   "source": [
    "## Option 3: Bag of Character N-Grams\n",
    "One way that we can potentially improve on our earlier feature representation is to use a bag of N-grams \\\n",
    "which encodes the occurence or lack-therof of different substrings from clinical encounter descriptions \\\n",
    "To construct this representation, we can again make use of the scikit-learn package's `CountVectorizer` class \\\n",
    "but instead of using the default `\"word\"` analyzer, we will instead use the `\"char_wb\"` analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df618f5e-6b3e-4218-9a42-4c17fc7adaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_option3(type2_data, non_type2_data):\n",
    "    print(\"Unifying and condensing records...\")\n",
    "    # now we will unify the data together into a single set of records\n",
    "    unified_records_type2 = get_unified_records(\n",
    "        type2_data['conditions'],\n",
    "        type2_data['observations'],\n",
    "        type2_data['medications'],\n",
    "        type2_data['procedures']\n",
    "    )\n",
    "\n",
    "    unified_records_non_type2 = get_unified_records(\n",
    "        non_type2_data['conditions'],\n",
    "        non_type2_data['observations'],\n",
    "        non_type2_data['medications'],\n",
    "        non_type2_data['procedures']\n",
    "    )\n",
    "\n",
    "    # now we will condense the records for the type 2 and non-type 2 patients\n",
    "    ##### MODIFICATION: Here we use spaces as a separator instead of '|' characters as we are working with n-grams\n",
    "    type2_condensed = get_condensed_record_data_for_ngrams(unified_records_type2)\n",
    "    non_type2_condensed = get_condensed_record_data_for_ngrams(unified_records_non_type2)\n",
    "\n",
    "    # now we will concatenate the two datasets together and label them\n",
    "    all_data_condensed = pd.concat([\n",
    "        type2_condensed.assign(LABEL=1),\n",
    "        non_type2_condensed.assign(LABEL=0)\n",
    "    ])\n",
    "\n",
    "    print(\"Vectorizing data...\")\n",
    "    # now we will get the multi-hot vector representation for the records\n",
    "    vectorizer = CountVectorizer(\n",
    "        binary=True,\n",
    "        analyzer='char_wb', ##### MODIFICATION: Use character word boundaries as the analyzer mode #####\n",
    "        ngram_range=(5, 5), ##### MODIFICATION: We want character 5-grams #####\n",
    "        lowercase=True ##### MODIFICATION: We want to treat \"Blood\" the same way as \"blood\" for example #####\n",
    "    )\n",
    "\n",
    "    # now vectorize the data to get a multi-hot representation\n",
    "    multi_hot_vectors = get_multihot_vector_representation(all_data_condensed, vectorizer, feature_col='EVENT_DESCRIPTION')\n",
    "\n",
    "    print(f\"Multi hot vectors have {multi_hot_vectors.shape[1]} features\")\n",
    "\n",
    "    print(\"Fitting and evaluating classifier...\")\n",
    "    # now we will construct the random forest classifier and evaluate it using KFold cross-validation\n",
    "    clf = LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "        max_iter=1000,\n",
    "        random_state=913,\n",
    "    )\n",
    "\n",
    "    # now we train and evaluate the classifier\n",
    "    results, fpr, tpr, cm = train_and_evaluate_classifier_kfold(\n",
    "        clf, multi_hot_vectors, all_data_condensed['LABEL'].to_numpy()\n",
    "    )\n",
    "\n",
    "    display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=results['AUROC'].iloc[-1], estimator_name='Logistic Regression - Pipeline 3')\n",
    "    display.plot()\n",
    "    plt.show()\n",
    "\n",
    "    # create and display the confusion matrix plot\n",
    "    display = ConfusionMatrixDisplay(cm, display_labels=np.array(['Non-Type 2', 'Type 2']))\n",
    "    display.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "\n",
    "    # now return the results, trained classifier, and vectorizer\n",
    "    return results, clf, vectorizer, pd.concat([unified_records_type2, unified_records_non_type2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b06209-b3da-4881-97c1-adb131877334",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p3, clf_p3, vectorizer_p3, all_records_p3 = pipeline_option3(type2_data_p2, non_type2_data_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a41ef4-80c6-40ca-accb-edf51bb2ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f68679-5f1c-4ab5-8f6f-cb61667d99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p3['AUROC'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c75688-981e-416e-b613-dbe546f1cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_p3 = get_feature_importance_rankings_ngram_lr(clf_p3, vectorizer_p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b41787-2b15-45a1-b511-2b276ba75da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_p3.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec7527-bb29-492f-8c8b-887458fb6be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records_p3[all_records_p3['DESCRIPTION'].str.contains('cont', regex=False, case=False)]['DESCRIPTION'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3c519-a382-4cb8-b810-1432228ccb3d",
   "metadata": {},
   "source": [
    "# Final Evaluation of Modeling Approaches on Held Out Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c591a60-354a-4971-9744-c4f17a81425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type2_test, non_type2_test = get_preprocessed_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca3a454-9997-41ca-8473-c0494ef87993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add additional preprocessing steps for the second pipeline\n",
    "type2_test_p2, non_type2_test_p2, _ = additional_preprocessing_steps(type2_test, non_type2_test, observation_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53ac56-9f59-4af2-af7c-be97df20c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline(type2_data, non_type2_data, clf, vectorizer, condenser, feature_col='EVENT_TOKEN'):\n",
    "    print(\"Unifying data for type 2 and non-type2 patients...\")\n",
    "    # now we will unify the data together into a single set of records\n",
    "    unified_records_type2 = get_unified_records(\n",
    "        type2_data['conditions'],\n",
    "        type2_data['observations'],\n",
    "        type2_data['medications'],\n",
    "        type2_data['procedures']\n",
    "    )\n",
    "\n",
    "    unified_records_non_type2 = get_unified_records(\n",
    "        non_type2_data['conditions'],\n",
    "        non_type2_data['observations'],\n",
    "        non_type2_data['medications'],\n",
    "        non_type2_data['procedures']\n",
    "    )\n",
    "    print(\"Converting to condensed representation...\")\n",
    "    # now we will condense the records for the type 2 and non-type 2 patients\n",
    "    type2_condensed = condenser(unified_records_type2)\n",
    "    non_type2_condensed = condenser(unified_records_non_type2)\n",
    "\n",
    "    # now we will concatenate the two datasets together and label them\n",
    "    all_data_condensed = pd.concat([\n",
    "        type2_condensed.assign(LABEL=1),\n",
    "        non_type2_condensed.assign(LABEL=0)\n",
    "    ])\n",
    "\n",
    "    print(\"Vectorizing data...\")\n",
    "\n",
    "    # now vectorize the data to get a multi-hot representation\n",
    "    multi_hot_vectors = get_multihot_vector_representation(all_data_condensed, vectorizer, train=False, feature_col=feature_col)\n",
    "\n",
    "    # print out the dimensionality of the multi_hot_vectors\n",
    "    print(f\"Multi hot vectors have {multi_hot_vectors.shape[1]} features\")\n",
    "\n",
    "    print(\"Evaluating classifier...\")\n",
    "    # evaluate the model on the validation fold\n",
    "    preds = clf.predict(multi_hot_vectors)\n",
    "    scores = clf.predict_proba(multi_hot_vectors)[:, 1]\n",
    "    test_y = all_data_condensed['LABEL'].to_numpy()\n",
    "    # get the AUROC\n",
    "    fpr, tpr, _ = roc_curve(test_y, scores)\n",
    "    auroc = auc(fpr, tpr)\n",
    "    # get the confusion matrix\n",
    "    cm = confusion_matrix(test_y, preds)\n",
    "    # save the metrics\n",
    "    metrics = {\n",
    "        'AUROC': auroc,\n",
    "        'Precision': cm[1, 1] / cm[:, 1].sum(),\n",
    "        'Recall': cm[1, 1] / cm[1].sum(),\n",
    "        'Specificity': cm[0, 0] / cm[0].sum()\n",
    "    }\n",
    "    \n",
    "\n",
    "    # create and display the ROC curve plot\n",
    "    display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=metrics['AUROC'], estimator_name='Pipeline 1 - Holdout Set')\n",
    "    display.plot()\n",
    "    plt.show()\n",
    "\n",
    "    # create and display the confusion matrix plot\n",
    "    display = ConfusionMatrixDisplay(cm, display_labels=np.array(['Non-Type 2', 'Type 2']))\n",
    "    display.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "\n",
    "    # now return the results, trained classifier, and vectorizer\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12306576-5eff-4581-bfc6-4ebf6af334ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p1_test = test_pipeline(type2_test, non_type2_test, clf, vectorizer, condenser=get_condensed_record_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af2871c-6bd3-49ef-9fa8-d4d983963ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f67759e-f24a-4d06-bb2e-ba9a41fc104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c204b7e-bace-4186-8b6e-8fd207f59489",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p2_test = test_pipeline(type2_test_p2, non_type2_test_p2, clf_p2, vectorizer_p2, condenser=get_condensed_record_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20340ff-c117-4b0f-89e9-a0b3eccc0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7995047-a8f1-4ad7-9a99-29846b8448f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40049d5-3c22-4968-98fd-ca4a289c9837",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p3_test = test_pipeline(\n",
    "    type2_test_p2, non_type2_test_p2, clf_p3, vectorizer_p3, \n",
    "    condenser=get_condensed_record_data_for_ngrams, feature_col='EVENT_DESCRIPTION'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a650752b-9eb9-47ad-b6dd-d3081d531486",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p3_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1dcabe-ca9f-4da7-9471-88a1a47e6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p3.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f454f6-38c4-4912-a440-5bbdaf05be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_collated = pd.DataFrame([\n",
    "    results_p1_test,\n",
    "    results_p2_test,\n",
    "    results_p3_test\n",
    "]).assign(pipeline=['Pipeline 1 - Bag of Labeled Encounters', 'Pipeline 2 - BOLE with Numeric Discretization', 'Pipeline 3 - Bag of Description n-grams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a81af0-65dd-448d-b5a6-3f161a67313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_collated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd438c6-1adb-438f-9cdb-8af06b7376c8",
   "metadata": {},
   "source": [
    "## Visualizing Difference in Distribution of Number of Distinct Events Between Positive (Type 2) and Negative (non-Type 2) patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ffea0d-17c6-4a08-b368-365b8cc1a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records_labeled = all_records.assign(\n",
    "    LABEL=lambda x: x['PATIENT'].isin(type2_patients).astype('int'),\n",
    "    EVENT_TOKEN=lambda x: x['CODE'].astype('str') + '::' + x['EVENT_TYPE']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a2984e-bba8-45ea-8c31-b1e006d8f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_encounter_type_counts = all_records_labeled.groupby('PATIENT').agg({\n",
    "    'EVENT_TOKEN': 'nunique',\n",
    "    'LABEL': 'max'\n",
    "}).reset_index().rename(columns={'EVENT_TOKEN': '# of distinct events'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846d7ad-7e68-46a0-a776-8f65fe152096",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_encounter_type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc2d3d-30cb-4177-b462-9f806206124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 200\n",
    "sns.histplot(patient_encounter_type_counts, x='# of distinct events', hue='LABEL', common_norm=False, stat='probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb602e-d8d8-4932-90bd-fb95a612f602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
